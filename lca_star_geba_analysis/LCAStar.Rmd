---
title: "LCA Star Methods"
author: "Niels W. Hanson"
date: "Friday, November 28, 2014"
output:
   html_document: 
      toc: true
      theme: readable
      highlight: default
csl: ieee.csl
bibliography: LCAStar.bib
---

```{r global_options, include=FALSE}
require(knitr)
opts_chunk$set(warning=FALSE, message=FALSE, dev = 'pdf')
```

## Overview 


Two simulated metagenomes were created using 10,000bp contigs, randomly sampled from a collection of 2713 genomes obtained from the NCBI(Downloaded March 15 2014 <ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/>) using the python script `subsample_ncbi.py`. The first simulation is a smaller sample of 100 genomes, sampling 10 random reads from each genome. The second simulation is larger, sampling a random subset of 2,000 genomes, sampling 10 random reads from each. Each simulation had its ORFs predicted and annotated against the RefSeq database using the MetaPathways pipeline [[@Konwar:2013cw, @Hanson2014]]. The original source of each contig was predicted from the taxonomic annotations ascribed to each LCA-predicted ORF using three different methods: $LCA^2$, $Simple Majority$, and our information-theoretic $LCA*$. $LCA^2$ simply applies the LCA algorithm again to the set of contig taxonomies. The $Simple Majority$ method ascribes the taxonomy of the contig to the taxonomy that has the greatest majority. Our $LCA*$ method applies the information-theoetic result and algorithm previously described with a majority theshold set to the default majority ($\alpha=0.5$).

We evaluated the performance of these predictions using two taxonomic distances on the NCBI taxonomy tree. First a simple walk on the NCBI Taxonomy Hierarchy from the observed predicted taxonomy to the original expected taxonomy. The second is a weighted taxonomic distance that weightes each edge proportional to $\frac{1}{d}$ where $d$ is the depth of the edge in the tree. See Hanson *et al.* (2014) for more details. The NCBI Taxonomy Hierarchy was modified with the additional *prokaryotes* node as a parent of *Bacteria* and *Archaea* nodes.

### Obtaining NCBI Genomes

* Downloaded `all.fna.tar.gz` and `summary.txt` from the NCBI's ftp server:ftp://ftp.ncbi.nlm.nih.gov/genomes/ Mar 12, 2014
* `summary.txt` contains some metadata for a number of these genomes:

```
Accession  GenbankAcc  Length	Taxid	ProjectID	TaxName	Replicon	Create Date	Update_Date
NC_000907.1	L42023.1	1830138	71421	57771	Haemophilus influenzae Rd KW20	chromosome 	Oct 19 2001	Sep 11 2013  4:30:20:076PM
```

* this kind of information would be useful for an analysis so I cross referenced all the Accession IDs with the actual `.fna` files that I had in `all.fna.tar.gz` with the following shell command:

```
cat summary.txt  | awk '{print $1}' > u
for s in `cat u`; do var1=`find . -name *${s%.[0-9]}*`; echo $s$'\t'$var1 >> ncbiID_to_file; done
```

* it turns out that this is a fairly comprehensive list 25 genomes were dropped because they were not found in the MetaData

```
grep --perl-regexp "\t$" ncbiID_to_file | wc
      25      25     324
grep --perl-regexp "\t$" ncbiID_to_file | wc
NC_003911.11  
AC_000091.1	
NC_009353.1	
NC_009444.1	
NC_010332.1	
NS_000190.1	
NC_011980.1	
NS_000196.1	
NS_000197.2	
NC_012627.1	
NC_012629.1	
NC_012630.1	
NC_012915.1	
NC_013416.1	
NC_013438.1	
NC_013597.1	
NC_013784.1	
NC_013785.1	
NC_013786.1	
NC_013787.1	
NC_013788.1	
NC_014629.1	
NC_015557.1	
NC_015587.1
```

* this leaves us with 2617 genomes with annotation of 2642, storing these in `ncbiID_to_file.txt`

```
grep --perl-regexp ".*fna" ncbiID_to_file2 | wc
    2617    5234  197129
wc ncbiID_to_file2
    2642    5259  197453 ncbiID_to_file2
grep --perl-regexp ".*fna" ncbiID_to_file2 > ncbiID_to_file.txt
```

### Creating Simulated Metagenomes

Wrote my own script `subsample_ncbi.py` quickly sample from a collection of fasta files specified in our `ncbiID_to_file.txt` that we created above.

The following example creates sub-sequences of length 10,000, sampling 10 sequences per file on a random subset of 100 (the random number generator is seeded so that results can be reproducible)

```
python subsample_ncbi.py -i ncbiID_to_file.txt -l 10000 -s 10 -n 100 -o lca_star_test1.fna
```

* lca_star_test1.fna is going to be our first test, we ran it through MetaPathways with the standard settings:

```
Run Date : 2014-03-15 
Nucleotide Quality Control parameters
  min length  180
ORF prediction parameters
  min length	60
  algorithm	prodigal
Amino acid quality control and annotation parameters
  min bit score	20
  min seq length	60
  annotation reference dbs	RefSeq_complete_nr_v62_dec2013
  min BSR	0.4
  max evalue	0.000001
Pathway Tools parameters
  taxonomic pruning 	no
rRNA search/match parameters
  min identity	20
  max evalue	0.000001
  rRNA reference dbs	GREENGENES_gg16S-2013-09-12
```

#### Second test

For a second test we sampled sequences of 10,000 bps using 10 subsamples from 2000 randomly selected genomes. Here, did the same procedure except with significantly more samples.

```
python ../../subsample_ncbi.py -i ../ncbiID_to_file.txt -o lca_test2.fasta -l 10000 -s 10 -n 2000
```

### Simulations and GEBA SAGs

* Load required libraries

```{r}
library(ggplot2)
library(reshape2)
library(dplyr)
theme_set(theme_bw()) # set theme and font
```

* read and merge the three datasets

```{r}
setwd("~/Dropbox/projects/LCAStar/lca_star_geba_analysis")
colClasses <- c("character", "character", "numeric", "numeric", "numeric",
                "character", "numeric", "numeric", "numeric", 
                "character", "numeric", "numeric", "character" )
geba_df <- read.table("GEBA_SAG_all_lcastar.txt", sep="\t", header=T, na.strings = "None", 
                      colClasses = colClasses, strip.white=TRUE, quote="")
test1_df <- read.table("test1_lcastar.txt", sep="\t", header=T, na.strings = "None", 
                      colClasses = colClasses, strip.white=TRUE, quote="")
test2_df <- read.table("test2_lcastar.txt", sep="\t", header=T, na.strings = "None", 
                      colClasses = colClasses, strip.white=TRUE, quote="")

all_df <- rbind(cbind(geba_df, Sample="GEBA"),cbind(test1_df, Sample="Small"),cbind(test2_df, Sample="Large"))

all_df$Sample <- factor(all_df$Sample, levels=c("Small", "Large", "GEBA"))
```

* compare p-value calculations between the Majority and LCA* methods

```{r fig.width=10.2, fig.height=3.65}
# p-value compare
g1 <- ggplot(all_df, aes(x=LCAStar_p, y=Majority_p)) 
g1 <- g1 + geom_point(aes(color=Sample), alpha=0.4)
g1 <- g1 + xlim(0,1)
g1 <- g1 + ylim(0,1)
g1 <- g1 + xlab("p-value (LCA*)")
g1 <- g1 + ylab("p-value (Majority)")
g1 <- g1 + facet_wrap(~ Sample)
g1 <- g1 + theme(legend.position="none")
g1
pdf(file = "pdfs/fig1.pdf", width = 10.2, height=3.65)
g1
dev.off()
```

**Figure 1:** Comparing p-values of taxonomic voting statistics Majority and LCA*.

Notes:

* all three samples exhibit the same pattern, suggesting that the simulation procecture is producing representative results at least in terms of the LCA* and Majority summary statistics 
* in many cases the simple Majority and LCA* solutions are the same, as shown by the strong linear trend
* when in disagreement LCA* has more favorable (lower) p-values than the simple majority, in many cases the p-value of the LCA* solution is significantly smaller than the simple majority one
* disagreements where $p_{Majority} < p_{LCA^*}$ occur when the LCA* transformation widens the difference between the first and second place leaders, although this is a bit of a corner case, and happens fairly infrequently
* the majority has a degenerate case where there is as tie in the $M = X_n$, which is a degenerate case, meaning that the Simple majority had to make an arbitary choise between two or more taxonomic candidates
* Suggests that if you subscribe to the p-value argument, LCA* produces more reliable results, although one can argue that his happens by construction.

Function to format and reshape data for ggplot:

```{r}
clean_up_data2 <- function(df) {
  new_df <-rbind(cbind(df$Contig, df$LCAStar, "LCAStar", df$Original, df$LCAStar_p, df$LCAStar_dist, df$LCAStar_WTD),
           cbind(df$Contig, df$Majority, "Majority", df$Original, df$Majority_p, df$Majority_dist, df$Majority_WTD),
           cbind(df$Contig, df$LCASquared, "LCASquared", df$Original, NA, df$LCASquared_dist, df$LCASquared_WTD))
  new_df <- as.data.frame(new_df)
  colnames(new_df) <- c("Contig", "Taxonomy", "Method", "Original", "p-value", "Walk", "WTD")
  new_df.m<- melt(new_df, id.vars = c("Contig", "Taxonomy", "Method", "Original"))
  new_df.m$value <- as.numeric(as.character(new_df.m$value))
  new_df.m$Method <- factor(new_df.m$Method, levels = c("LCASquared", "Majority", "LCAStar"))
  new_df.m
}
```

```{r}
# reshape data into ggplot form
test1_df.m <- clean_up_data2(test1_df)
test1_df.m<- cbind(test1_df.m, Sample="Small")
test2_df.m <- clean_up_data2(test2_df)
test2_df.m <- cbind(test2_df.m, Sample="Large")
geba_df.m <- clean_up_data2(geba_df)
geba_df.m <- cbind(geba_df.m, Sample="GEBA")

all_df.m <- rbind(test1_df.m, test2_df.m, geba_df.m)
```

* plot the distribution of distances between predictions and taxonomic origin

```{r fig.height=7, fig.width=7}
# plotting parameters
my_line_col = "#4C4C4C"
line_size = 1.2
alpha_val = 0.5

walk_means <- select(all_df.m, Method, Sample, variable, value) %>%
  group_by(Method, Sample, variable) %>%
  filter(variable== "Walk") %>%
  summarize(mean=mean(value, na.rm = T),
            percent_25 = quantile(value, probs = 0.25, na.rm = T),
            median = quantile(value, probs=0.5, na.rm = T),
            percent_75 = quantile(value, probs = 0.75, na.rm = T))

g2 <- ggplot(subset(all_df.m, variable == "Walk"), aes(x=value, fill=Method))
# g2 <- g2 + geom_histogram(binwidth=2, alpha=0.8) 
g2 <- g2 + geom_density(adjust=5, alpha=alpha_val) 
g2 <- g2 + facet_grid(Method ~ Sample, scales = "free_x")
g2 <- g2 + theme(legend.position="none")
g2 <- g2 + geom_vline(aes(xintercept=mean), colour=my_line_col, size=line_size, alpha=alpha_val, walk_means)
g2 <- g2 + geom_vline(aes(xintercept=median), colour=my_line_col, linetype="dashed", size=line_size, alpha=alpha_val, walk_means)
g2 <- g2 + xlab("Simple-walk Distance")
g2 <- g2 + ylab("Density")
g2
pdf(file = "pdfs/fig2.pdf", width = 7, height=7)
g2
dev.off()
```

**Figure 2:** Distribution of simple-walk distances of taxonomic predictions to contig origin: LCASquared, Majority, LCA*. Kernel densities for error distances from each of the three prediction methods on the two simluated and GEBA datasets.

Notes:

* In all cases, LCASquared perfomed on-aggregate more poorly than the voting-based methods, Majority and LCAStar.
   * Means and Medians of predicted distances for LCASqared were consistently larger
* LCA* and Majority by large performed very simmialrly in terms of distance from the target taxon, in fact in many cases they are one and the same when no entropy-based transformations need to occur
   * Majority method has a slightly longer tail in the GEBA sample
   * medians are identical in all samples, better measure in slightly skewed distribution
* Simple-walk distance is a crude measure of taxonomic distance

```{r include=FALSE}
# original histograms
g2a <- ggplot(subset(all_df.m, variable == "Walk"), aes(x=value, fill=Method))
g2a <- g2a + geom_histogram(binwidth=2, alpha=alpha_val) 
# g2a <- g2a + geom_density(adjust=6, alpha=0.8) 
g2a <- g2a + facet_grid(Method ~ Sample, scales = "free")
g2a <- g2a + theme(legend.position="none")
#g2a <- g2a + geom_vline(aes(xintercept=percent_25), colour=my_line_col, size=1.5, alpha=0.5, means)
#g2a <- g2a + geom_vline(aes(xintercept=median), colour=my_line_col, size=1.5, alpha=0.5, means)
#g2a <- g2a + geom_vline(aes(xintercept=percent_75), colour=my_line_col, size=1.5, alpha=0.5, means)
g2a <- g2a + xlab("Simple Distance")
g2a <- g2a + ylab("Frequency")
g2a
```

Same thing again, expect using the weighted taxonomic distance.

```{r fig.height=7, fig.width=7}
means <- select(all_df.m, Method, Sample, variable, value) %>%
  group_by(Method, Sample, variable) %>%
  filter(variable== "WTD") %>%
  summarize(mean=mean(value, na.rm = T),
            percent_25 = quantile(value, probs = 0.25, na.rm = T),
            median = quantile(value, probs=0.5, na.rm = T),
            percent_75 = quantile(value, probs = 0.75, na.rm = T))

g3 <- ggplot(subset(all_df.m, variable == "WTD"), aes(x=value, fill=Method))
g3 <- g3 + geom_density(adjust=20, alpha=0.6) 
g3 <- g3 + facet_grid(Method ~ Sample, scales = "free_y")
g3 <- g3 + theme(legend.position="none")
g3 <- g3 + geom_vline(aes(xintercept=mean), colour=my_line_col, size=1.5, alpha=0.5, means)
g3 <- g3 + geom_vline(aes(xintercept=median), colour=my_line_col, linetype="dashed", size=1.5, alpha=0.5, means)
g3 <- g3 + xlab("Weighted Taxonomic Distance")
g3 <- g3 + ylab("Density")
g3
pdf(file = "pdfs/fig3.pdf", width = 7, height=7)
g3
dev.off()
```

**Figure 3:** Distribution of WTD distances of taxonomic predictions to contig origin: LCASquared, Majority, LCA*. Kernel densities for error distances from each of the three prediction methods on the two simluated and GEBA datasets.

Notes:

* distance punishes divergent predictions more than in-lineage predictions
* LCA* and Majority methods again consistently performed better than LCASquared, although difference is not as extreme as by definition LCASquared will have to predict a common ancestor to all electorate
* LCASquared occassionally has an exetremely large tail of 'root' predictions
* Voting based methods have essentially identical median and average
    * Majority in GEBA sample has slightly shorter tail

```{r include=FALSE}
g3a <- ggplot(subset(all_df.m, variable == "WTD"), aes(x=value, fill=Method))
g3a <- g3a + geom_histogram(binwidth=0.1, alpha=alpha_val) 
g3a <- g3a + facet_grid(Method ~Sample, scales = "free_y")
g3a <- g3a + theme(legend.position="none")
g3a <- g3a + geom_vline(aes(xintercept=mean), colour=my_line_col, size=line_size, alpha=alpha_val, means)
g3a <- g3a + geom_vline(aes(xintercept=median), colour=my_line_col, linetype="dashed", size=line_size, alpha=alpha_val, means)
g3a <- g3a + xlab("Weighted Taxonomic Distance")
g3a <- g3a + ylab("Frequency")
g3a
```

Treating the whole thing as a regression problem we have the following global averages.

```{r}
res <- select(all_df.m, Method, Sample, variable, value) %>%
       filter(variable %in% c("Walk", "WTD")) %>%
       group_by(Method, Sample, variable) %>%
       summarize(n_obs = n(), RMSE = mean(abs(value), na.rm=TRUE))
g4 <- ggplot(res, aes(y=RMSE, x =Method, fill=Method)) 
g4 <- g4 + geom_bar(stat="identity") 
g4 <- g4 + facet_grid(variable~Sample, scales="free_y")
g4 <- g4 + theme(legend.position="none")
g4
pdf(file = "pdfs/fig4.pdf", width = 7, height=7)
g4
dev.off()
```

**Figure 4:** RMSE Values for our prediction errors in the simple Walk and WTD.

```{r include=FALSE}
g8 <- ggplot(subset(geba_df.m, variable == "Walk"), aes(x=value, fill=Method))
g8 <- g8 + geom_histogram(binwidth=2, alpha=0.8) 
g8 <- g8 + facet_wrap( ~ Method, ncol = 1)
g8 <- g8 + theme(legend.position="none")
g8 <- g8 + xlim(0,15)
g8 <- g8 + xlab("Value")
g8 <- g8 + ylab("Frequency")
g8

g9 <-ggplot(subset(geba_df.m, variable == "WTD"), aes(x=value, fill=Method))
g9 <- g9 + geom_histogram(binwidth=0.1, alpha=0.8) 
g9 <- g9 + facet_wrap( ~ Method, ncol = 1)
g9 <- g9 + theme(legend.position="none")
g9 <- g9 + xlim(-1.5,0)
g9 <- g9 + xlab("Value")
g9 <- g9 + ylab("Frequency")
g9

# g11 <-ggplot(subset(geba_df.m, variable %in% c("WTD", "Walk")), aes(x=Method, y=value, fill=Method))
# g11 <- g11 + geom_violin(alpha=0.6)
# g11 <- g11 + scale_y_log10()
# g11 <- g11 + coord_flip()
# g11 <- g11 + facet_wrap( ~ variable)
# g11
geba_df.m$Taxonomy <- as.vector(geba_df.m$Taxonomy)
geba_df.m$Original<- as.vector(geba_df.m$Original)
res <- select(geba_df.m, Method, Taxonomy, Original) %>%
       group_by(Method) %>%
       summarize(n_obs = n(), accuracy = sum(Taxonomy == Original)/ length(Taxonomy))

g12 <- ggplot(res, aes(x=Method, y=accuracy, fill=Method)) 
g12 <- g12 + geom_bar(stat="identity", alpha=0.8)
g12 <- g12 + theme(legend.position="none")
g12
```

## References

